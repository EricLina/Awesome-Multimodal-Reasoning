# Awesome-Multimodal-Reasoning

**Contributions are most welcome**, if you have any suggestions or improvements, feel free to create an issue or raise a pull request.

## Contents
 - [Benchmark](#Visual-Reasoning-Benchmark)
 - [Supervised Fine-Tuning](#Supervised-Fine-Tuning)
 - [Reinforcement Learining](#Reinforcement-Learining)
 - [SFT + RL](##SFT+RL)

## Multimodal Reasoning Benchmark

- [**M**3**CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought**](https://arxiv.org/html/2405.16473v1)
- [MME-CoT üî•üïµÔ∏è: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency](https://github.com/CaraJ7/MME-CoT)

## Supervised Fine-Tuning
### Image MLLM
- [Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904)
- [LLaVA-CoT: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440)
### Video MLLM

## Reinforcement Learining

### Image MLLM
- [InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model](https://arxiv.org/abs/2501.12368)
- [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/abs/2411.10442)
- [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/abs/2502.10391)
- [OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference](https://github.com/PhoenixZ810/OmniAlign-V)

- [VLM-R1](https://github.com/om-ai-lab/VLM-R1/tree/main?tab=readme-ov-file)
- [Unified Reward Model for Multimodal Understanding and Generation](https://arxiv.org/abs/2503.05236)
- [MM-EUREKA: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning](https://github.com/ModalMinds/MM-EUREKA)
- [R1-Zero‚Äôs ‚ÄúAha Moment‚Äù in Visual Reasoning on a 2B Non-SFT Model](https://arxiv.org/abs/2503.05132)
- [Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement](https://github.com/dvlab-research/Seg-Zero)
- [Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2503.06749)

### Video MLLM

- [Open-R1-Video](https://github.com/Wang-Xiaodong1899/Open-R1-Video)
- [Video-R1](https://github.com/tulerfeng/Video-R1)

- [Temporal Preference Optimization for Long-Form Video Understanding](https://arxiv.org/abs/2501.13919)

## SFT+RL

### Image MLLM
- [Improve Vision Language Model Chain-of-thought Reasoning](https://arxiv.org/pdf/2410.16198)

### Video MLLM
- [Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding]()
- [video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model](https://arxiv.org/abs/2502.11775)



## Other

[Multimodal Chain-of-Thought Reasoning in Language Models](https://github.com/amazon-science/mm-cot)
