# Awesome-Multimodal-Reasoning

**Contributions are most welcome**, if you have any suggestions or improvements, feel free to create an issue or raise a pull request.

## Contents
 - [Benchmark](#Visual-Reasoning-Benchmark)
 - [Model](#Reinforcement-Learining)
 - [Data](##SFT+RL)



## Benchmark

| Date | Project                                                      | Task |
| ---- | ------------------------------------------------------------ | ---- |
|      | [**M**3**CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought**](https://arxiv.org/html/2405.16473v1) |      |
|      | [[MME-CoT üî•üïµÔ∏è: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency](https://github.com/CaraJ7/MME-CoT)](https://arxiv.org/pdf/2410.16198) |      |

## Model

### Image MLLM

| Date  | Project                                                      | SFT  | RL   | Task |
| ----- | ------------------------------------------------------------ | ---- | ---- | ---- |
|       | [Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904) |      |      |      |
|       | [Improve Vision Language Model Chain-of-thought Reasoning](https://arxiv.org/pdf/2410.16198) |      |      |      |
|       | [LLaVA-CoT: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440) |      |      |      |
| 24.11 | Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization [[Paper]](https://arxiv.org/abs/2411.10442) [[Code]()] |      |      |      |
| 25.01 | InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model [[Paper]](https://arxiv.org/abs/2501.12368) [[Code]]() |      |      |      |
| 25.02 | MM-RLHF: The Next Step Forward in Multimodal LLM Alignment [[Paper]](https://arxiv.org/abs/2502.10391) |      |      |      |
|       | OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference [[Code]](https://github.com/PhoenixZ810/OmniAlign-V) |      |      |      |
|       | VLM-R1 [[Code]](https://github.com/om-ai-lab/VLM-R1/tree/main?tab=readme-ov-file) |      |      |      |
| 25.03 | Unified Reward Model for Multimodal Understanding and Generation [[Paper]](https://arxiv.org/abs/2503.05236) |      |      |      |
|       | MM-EUREKA: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning [[Code]](https://github.com/ModalMinds/MM-EUREKA) |      |      |      |
| 25.03 | R1-Zero‚Äôs ‚ÄúAha Moment‚Äù in Visual Reasoning on a 2B Non-SFT Model [[Paper]](https://arxiv.org/abs/2503.05132) |      |      |      |
|       | Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement [[Code]](https://github.com/dvlab-research/Seg-Zero) |      |      |      |
| 25.03 | Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models [[Paper]](https://arxiv.org/abs/2503.06749) |      |      |      |
| 25.03 | Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning [[Paper]](https://arxiv.org/pdf/2503.07065) |      |      |      |
| 25.03 | LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL [[Paper]](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2503.07536) |      |      |      |
| 25.03 | VisRL: Intention-Driven Visual Perception via Reinforced Reasoning [[Paper]](https://arxiv.org/pdf/2503.07523) |      |      |      |
| 25.03 | GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks [[Paper]](https://arxiv.org/pdf/2503.06514) |      |      |      |



### Video MLLM

| Date | Project                                                      | Comment |
| ---- | ------------------------------------------------------------ | ------- |
|      | Open-R1-Video[[Code]](https://github.com/Wang-Xiaodong1899/Open-R1-Video) |         |
|      | Video-R1[[Code]](https://github.com/tulerfeng/Video-R1)      |         |
|      | Temporal Preference Optimization for Long-Form Video Understanding [[Paper]](https://arxiv.org/abs/2501.13919) |         |
|      | [Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding]() |         |
|      | [video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model](https://arxiv.org/abs/2502.11775) |         |



### Image Generation

TODO

### LLM

| Date | Project                                                      | Comment |
| ---- | ------------------------------------------------------------ | ------- |
|      | [Multimodal Chain-of-Thought Reasoning in Language Models](https://github.com/amazon-science/mm-cot)https://github.com/Wang-Xiaodong1899/Open-R1-Video) |         |



## Data
| Date | Project                                                      | Comment |
| ---- | ------------------------------------------------------------ | ------- |
|      |  |         |

